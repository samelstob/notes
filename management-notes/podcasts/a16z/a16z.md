# a16z

# Making Sense of Big Data, Machine Learning, and Deep Learning

* "The 3 V's (volume, variety, velocity) - that definition is functionally
  correct but focusses on the problems of Big Data - the challenges you have
to deal with when you deal with big data, but it misses the question of why
you do you want to deal with this data.  The reason for big data is machine
learning."

* "Peter Norvig, research at Google, Big Data is not just quantatively
  different, it's qualatitvely different."
* "Not so much data do you have but how much data is enough to learn from?"
* "Google hav elearned to take advantage of big data to take a lot of
  decisions"

* "We tend not to want to have problems unless their is a greater benefit to
  that cost.  The benefit of big data is because we can unleash algorithms at
them, and these algorithms can automatically detect patterns."

"Wisdom - you have the rules, but you know when the rules don't apply because
you've seen 3, 4, 5 corner cases before.  Somehow, "intuitively" you find in
this situation the rule doesn't apply, but what we think of as intuition you
can think of as parameters inside a machine learning model."

"BI - sometimes we are captured by what we said in the past.  What was
available was to look backward - aggregations - "How much revenue did we make
yetserday from this region.

BI going forward the ability to apply machine learning to big data.  With
enough data from all of that experiences we can build a model from that and
project into the future.  Not just look at past questions but also future
quesitons - predict the unknowns from the knowns."

"Top down view, bottom up view.

Bottom up view - that's how technology develops. We always build things from the bottom up and then realise there's a pattern a look
top down again.

From the bottom up view Hadoop is mainly primarily a storage layer.  The
distinction between this and other file systems - it's highly parallel
(resiliency) and it's capable of running on commodity hardware.

For the first time people can afford to buy terabytes of storage and only pay
a little for it.

Economy of hardware, Companies showing that there is value in taking all this
data and making decisions from it, Open Source

Spark - If you start from the storage layer - storing is not enough

- Bottom layer - Storage - Big Data
- Middle layer - Compute - Big Compute - e.g. MapReduce

In any computing stack you need more than just storage.

"Hadoop MapReduce - take all the data, do some computation, put it back

Unfortunately MapReduce is not designed originally to handle queries.

"MapReduce was designed to be slow.  MapReduce as implemented at Google by
Jeff Dean and Sanjay Gamawat back in the early 2000's.  The MapReduce engine
was intended to do one particularly job - crawl and index the web.

Google's approach was to parallise it over 1000's of machines.  When you have
1000's of commodity machines doing a task that may last half a day, the
probability of one of those machines going down the probability of one of
those machines going down approaches one.

When a machine goes down the question comes up "Do we start the job over".
It's designed in a way that if a single machine goes down, another machine can
pick up where it started.

Spark takes a different approach.  Spark's goal is to be able to do a lot of
the queries very fast.

Fast is competitiveness - you get there before I do and you win.

When you can something in "real time" you will change your workflow.

If the user can't get something done in 5 seconds they won't ever do it.

So far our industry has not built the bridge from the above to the human
user.

Big Apps on top of Big Compute on top of Big Data

Q. We went from 500ms to 100ms - what happens when we get to zero?
A. Why stop at zero?"

## a16z Podcast: For Google, Android is a Tactic and Cloud is a Strategy 

http://a16z.com/2015/05/29/a16z-podcast-for-google-android-is-a-tactic-and-cloud-is-a-strategy/
